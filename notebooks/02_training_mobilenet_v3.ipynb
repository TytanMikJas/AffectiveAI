{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1efe1a",
   "metadata": {},
   "source": [
    "# MobileNetV3 fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b7fe3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b320adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from src.data.loaders import KDEFDataModule\n",
    "from src.models.trainer import EmotionClassifier\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f9a1a",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d9065",
   "metadata": {},
   "source": [
    "#### We discard images of angles `FullRight` and `FullLeft` and bugged files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ca4b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped buggy file: data\\raw\\AF01\\AF01SUFR.JPG\n",
      "Skipped buggy file: data\\raw\\AF10\\AF10AFFR.JPG\n",
      "Skipped buggy file: data\\raw\\AF11\\AF11NEHL.JPG\n",
      "Skipped buggy file: data\\raw\\AF20\\AF20DIHL.JPG\n",
      "Skipped buggy file: data\\raw\\AM25\\AM25DIFL.JPG\n",
      "Skipped buggy file: data\\raw\\AM34\\AM34DIFR.JPG\n",
      "Skipped buggy file: data\\raw\\BF13\\BF13NEHR.JPG\n",
      "Skipped buggy file: data\\raw\\BM21\\BM21DIFL.JPG\n",
      "Skipped buggy file: data\\raw\\BM22\\BM22DIHL.JPG\n",
      "Skipped buggy file: data\\raw\\BM24\\BM24DIFL.JPG\n"
     ]
    }
   ],
   "source": [
    "BUGGED_FILES = [\n",
    "    \"AF01SUFR\",\n",
    "    \"AF10AFFR\",\n",
    "    \"AF11NEHL\",\n",
    "    \"AF20DIHL\",\n",
    "    \"AM25DIFL\",\n",
    "    \"AM34DIFR\",\n",
    "    \"BF13NEHR\",\n",
    "    \"BM21DIFL\",\n",
    "    \"BM22DIHL\",\n",
    "    \"BM24DIFL\",\n",
    "]\n",
    "target_angles = [\"S\", \"HL\", \"HR\"]\n",
    "all_files = []\n",
    "\n",
    "for p in Path(\"data/raw\").rglob(\"*.JPG\"):\n",
    "    name = p.stem\n",
    "    angle = name[6:]\n",
    "\n",
    "    if name in BUGGED_FILES:\n",
    "        print(\"Skipped buggy file:\", p)\n",
    "        continue\n",
    "\n",
    "    if angle in target_angles:\n",
    "        all_files.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87420a14",
   "metadata": {},
   "source": [
    "#### We perform a subject-wise split to ensure that images from the same subject do not appear in different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45df08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 2936\n",
      "Number of unique subjects: 70\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"path\": all_files})\n",
    "\n",
    "df[\"subject_id\"] = df[\"path\"].apply(lambda x: x.name[1:4])\n",
    "df[\"session\"] = df[\"path\"].apply(lambda x: x.name[0])\n",
    "\n",
    "print(f\"Number of images: {len(df)}\")\n",
    "print(f\"Number of unique subjects: {df['subject_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db31d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=0)\n",
    "train_idx, temp_idx = next(splitter.split(df, groups=df[\"subject_id\"]))\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "temp_df = df.iloc[temp_idx]\n",
    "\n",
    "splitter_val = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "val_idx, test_idx = next(splitter_val.split(temp_df, groups=temp_df[\"subject_id\"]))\n",
    "\n",
    "val_df = temp_df.iloc[val_idx]\n",
    "test_df = temp_df.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a44e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train set: 2181 images (Subjects: 52)\n",
      "Val set:   377 images (Subjects: 9)\n",
      "Test set:  378 images (Subjects: 9)\n",
      "------------------------------\n",
      "Intersection Train/Val: set()\n",
      "Intersection Train/Test: set()\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "print(\n",
    "    f\"Train set: {len(train_df)} images (Subjects: {train_df['subject_id'].nunique()})\"\n",
    ")\n",
    "print(f\"Val set:   {len(val_df)} images (Subjects: {val_df['subject_id'].nunique()})\")\n",
    "print(f\"Test set:  {len(test_df)} images (Subjects: {test_df['subject_id'].nunique()})\")\n",
    "\n",
    "train_ids = set(train_df[\"subject_id\"].unique())\n",
    "val_ids = set(val_df[\"subject_id\"].unique())\n",
    "test_ids = set(test_df[\"subject_id\"].unique())\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Intersection Train/Val: {train_ids.intersection(val_ids)}\")\n",
    "print(f\"Intersection Train/Test: {train_ids.intersection(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94a4fb",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c7e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Backbone frozen. Training classifier head only.\n"
     ]
    }
   ],
   "source": [
    "data_module = KDEFDataModule(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"outputs/models\",\n",
    "    filename=\"mobilenet_v3_kdef-frozen-{epoch:02d}-{val_f1:.2f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_f1\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "model_frozen = EmotionClassifier(num_classes=7, learning_rate=1e-3, freeze_backbone=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=CSVLogger(\"outputs\", name=\"logs\"),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bfc297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ MobileNetV3               │  4.2 M │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ train_acc     │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ val_acc       │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ val_f1        │ MulticlassF1Score         │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_precision │ MulticlassPrecision       │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ val_recall    │ MulticlassRecall          │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ conf_mat      │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
       "└───┴───────────────┴───────────────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ MobileNetV3               │  4.2 M │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ train_acc     │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ val_acc       │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ val_f1        │ MulticlassF1Score         │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_precision │ MulticlassPrecision       │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ val_recall    │ MulticlassRecall          │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ conf_mat      │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
       "└───┴───────────────┴───────────────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.2 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 3.0 M                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.2 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 16                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 261                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.2 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 3.0 M                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 4.2 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 16                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 261                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094099a60a9440e1975f0e821eb953c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model_frozen, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf92074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "model_unfrozen = EmotionClassifier(num_classes=7, learning_rate=1e-3, freeze_backbone=False)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"outputs/models\",\n",
    "    filename=\"mobilenet_v3_kdef-unfrozen-{epoch:02d}-{val_f1:.2f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_f1\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=CSVLogger(\"outputs\", name=\"logs\"),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36bf4ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\studia\\AffectiveAI\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:881: Checkpoint directory D:\\studia\\AffectiveAI\\outputs\\models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ MobileNetV3               │  4.2 M │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ train_acc     │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ val_acc       │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ val_f1        │ MulticlassF1Score         │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_precision │ MulticlassPrecision       │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ val_recall    │ MulticlassRecall          │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ conf_mat      │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
       "└───┴───────────────┴───────────────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ MobileNetV3               │  4.2 M │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ train_acc     │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ val_acc       │ MulticlassAccuracy        │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ val_f1        │ MulticlassF1Score         │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_precision │ MulticlassPrecision       │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ val_recall    │ MulticlassRecall          │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ conf_mat      │ MulticlassConfusionMatrix │      0 │ train │     0 │\n",
       "└───┴───────────────┴───────────────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.2 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.2 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 16                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 261                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.2 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.2 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 16                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 261                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dbc7158f744c59903c11c0c153acb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model_unfrozen, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "affective-ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
